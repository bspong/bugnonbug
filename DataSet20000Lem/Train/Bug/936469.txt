Investigate TART ".error" regression on UX branch on OS X 10.8
mconley recently noticed a regression where some TART .error value on OS X 10.8 on the UX branch are considerably higher than the m-c one (reminder: the .error value indicate target v actual animation duration, e.g. if the animation should last 200ms but in practice last 207, then .error is 7):

http://compare-talos.mattn.ca/breakdown.html?oldTestIds=31243565,31252017,31252031,31252075,31252083,31252091,31252101,31252127,31252135,31252145&newTestIds=31242563,31252259,31252363,31252409,31252547,31252555,31252567,31252593,31252601,31252629&testName=tart&osName=Mac%2010.8&server=graphs.mozilla.org

The most meaningful one are (highest %/absolute regressions):
- simple-close-DPI1.error where m-c is 0.77 while UX is 7.19 (830%)
- icon-open-DPI1.error where m-c is 19.7 while UX is 34.7 (15ms)


Initially we thought they've been there all along and we somehow dismissed them, but looking at a similar comparison from Aug-25 show that the regression were considerably smaller two month ago - and they were indeed dismissible:

http://compare-talos.mattn.ca/breakdown.html?oldTestIds=29935085,29943989,29944011,29944037,29944043,29944049,29944065,29944073,29944079,29944143&newTestIds=29935065,29944097,29944113,29944119,29944125,29944149,29944155,29944161,29944167,29944191&testName=tart&osName=Mac%2010.8&server=graphs.mozilla.org

Where:
- simple-close-DPI1.error where m-c is 4.7 while UX is 6.4
- icon-open-DPI1.error where m-c is 33.3 while UX is 34.5


Datazilla show this (the graph start on Sep): https://datazilla.mozilla.org/?start=1379500722&stop=1383236782&product=Firefox&repository=Firefox&os=mac&os_version=OS%20X%2010.8&test=tart&compare_product=Firefox&compare_repository=UX&project=talos

The .error result are very noisy and hard to read/interpret, but:
- We didn't notice a meaningful change over time of the .error value on UX/m-c.
- Of the above two .error values, _maybe_ UX is worse than m-c despite the noise - about twice a high - but it could be just the way datazilla overlay the two set one over the other. Hard to tell.


So the question is what cause the higher .error regression in recent build compared to two month ago - a indicated by the above compare-talos links, and possibly also by datazilla.

Jeff suggested that it might be noise from +1 frame to complete the animation due to some recent gfx changes, since the first high regression is about 7ms, which is similar to a single frame duration of that animation. Such regression would be acceptable IMO.

It could also be due to the relatively high noise of the .error values.

However, considering that each talos TART run executes each test 25 times, and both comparison supposedly include 10 talos TART run (the link do seem to indicate this), then if there's a 1-frame noise, it should average over so many executions, and not show up. But it seems it doe show.


So step one is to verify/reproduce the result locally. I executed TART locally on a 2010 MBA (OS X 10.8.4). I've used official UX and m-c nightlies from 2013-11-07 with clean profiles. I installed the TART v1.5 addon from bug 848358, and set the prefs to make it similar to the talos setup (ASAP mode, OMTC off).

Here are my result of the test which showed the highest regression on compare talos (I ran the test more than once, restarted the browser in between, with different number of repetition etc, and all the results/runs were in the same ballpark):

m-c 2013-11-07 (30 runs):
-------------------------
simple-close-DPI1.half.TART Average (30): 8.44 stddev: 2.97
simple-close-DPI1.all.TART Average (30): 10.55 stddev: 3.72
simple-close-DPI1.error.TART Average (30): 9.63 stddev: 6.90 <--

icon-open-DPI1.half.TART Average (30): 10.45 stddev: 2.05
icon-open-DPI1.all.TART Average (30): 13.85 stddev: 2.35
icon-open-DPI1.error.TART Average (30): 36.03 stddev: 5.92 <--


UX 2013-11-07 (30 runs):
------------------------
simple-close-DPI1.half.TART Average (30): 7.75 stddev: 3.33
simple-close-DPI1.all.TART Average (30): 8.94 stddev: 3.60
simple-close-DPI1.error.TART Average (30): 11.18 stddev: 5.17 <--

icon-open-DPI1.half.TART Average (30): 10.31 stddev: 1.34
icon-open-DPI1.all.TART Average (30): 11.96 stddev: 1.23
icon-open-DPI1.error.TART Average (30): 37.16 stddev: 7.67 <--


While the .error were slightly worse on UX on my local runs, it's nowhere near the indication we've had initially (highest of those is ~1.5ms = ~15%). Also, roughly, the stddev of the .error value is ~70% of 1 frame interval of that animation - which sound reasonable considering that the number of frame might differ by 1 between runs.


Since the local result were relatively consistent, then maybe:

1. Compare talos doesn't average all the run properly.

2. One/some of the build which the compare-talos link used were not representative.

3. When Datazilla overlay two set of result onto the same graph, it first draw one set, then the other on top of it, which may lead to incorrect interpretation of the graphs.

4. Maybe the noise is between build (rather than between runs), and the two UX/mc build which I tested just happen to perform similarly.

5. There is a consistent UX regression (on average) which, for some reason, didn't show on my local runs.

6. Other ideas?


I think we should:
1. Check the value which compare-talos used for those links.
2. Get the datazilla value and calculate the stats manually.

If we're lucky and the value from these two set indicate result similar to my local runs, then we're good (1.5ms .error regression on average is negligible/not a blocker), but it'd still be nice to fix compare-talos and think what to do about datazilla's display.

Otherwise, we'll need to investigate if there's a between-builds noise (so datazilla's interpretation is correct, my local run wa lucky builds, and compare-talos wa unlucky builds), and possibly the source of the regression-on-average.
