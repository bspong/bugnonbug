Firefox ha segmentation fault (crashes) and fails in strange way with limited memory
User-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.12) Gecko/20051205 Epiphany/1.6.4
Build Identifier: Firefox 1.5.0.1

If one performs perfectly reasonable memory stress testing on current "production" version of Firefox it fails in a variety of ways, the most obvious of which is "crashing" with a segmentation fault.

There are indication that little or no stress testing ha been done on Firefox with constrained memory. Doing this is one obvious way to reveal bug which may be present in the basic system code.

Reproducible: Always

Steps to Reproduce:
1. ulimit -Sv 53000
2. /usr/bin/firefox (with a minimal profile, e.g. home = about:blank)
3. Enter URL: http://www.nytimes.com/


Actual Results:
Firefox crash and start the bug reporting agent.

One want to limit the available virtual memory to just slightly above that required to start firefox and test the limit of various functions. If the available memory is set too low one cannot map the library but at slightly above that one should be able to "stress" all various aspect of memory allocation/management.

Attempting to test various option I have seen at least the following problems:
1. Crashes on opening pages.
2. Crashes on Bookmarks >> Manage Bookmarks (with only about 10 small bookmarks)
3. Fails to open a functional new window (Ctrl-N will open a window but you can't use it for navigation, it end up with a grey background rather than a white background -- in theory one shouldn't open a non-functional window).
4. Specifying a new URL will allow navigation to a new page but will not navigate back (hitting the back button doe nothing).



Expected Results:
1) Never crash.
2) Not perform an action which is useless (e.g. opening a window which cannot be used, warn if navigating forward will disallow navigating back, etc).
3) Fail gracefully for excessive request (e.g. if one want to display a huge image but can't allocate memory for it, one should still be able to figure out how big the image would be, properly tell the display manager to block out that space, and provide some text in it place, e.g. "Place for 10MB image 'my-really-big-img.jpg'"

The fact that these bug exist at this state of Firefox development and "production" release show an extreme disregard for stress testing, particularly with regard to memory usage. When problem started being reported with Firefox memory usage (some of which are memory leak but many of which are heap fragmentation problems) one of the first question which should have been examined wa "Does Firefox fail gracefully under severe memory constraints?" It doe not appear that that ha been done.

The reason for this is a follows. Most people are not going to bother watching Firefox memory usage (using top / vmstat / System Monitor). Instead they might want to receive "reasonable" warning when memory consumption ha gotten out of hand (Linux system performance go down the tube when the Firefox resident memory requirement exceed 60-70% of physical memory). Though this is a Linux paging problem, it is ascerbated by Firefox's poor heap memory management causing execessive paging. One obvious solution is to limit Firefox so it virtual memory requirement can never exceed 50-60% of physical memory. This is what the ulimits are for (one could even have the script /usr/bin/firefox preset the virtual memory limit!). If Firefox then warned the user -- but did *not* fail or exit -- the user would (a) be aware of page which may be memory hogs; and (b) choose to continue browsing with perhaps some degradation in performance or (c) restart Firefox to reinitialize (compactify) the heap.

The flexibility of constraining Firefox using the Linux Soft & Hard memory limit is a very reasonable way to manage the browser. (I.e. there should be some config flag to allow Firefox to push the soft limit up to the hard limits.) [I suspect the ability to set ulimits is also available with Solaris and perhaps even Mac OS X -- so this approach would only not work under Windows.]

Tests under memory constraint should include:
1) Browsing to a new page (when memory for the "history" allocation fails).
2) Opening "Manage Bookmarks" under tight constraint (use Firefox with an empty bookmark file, then replace it with one which is 2-5MB).
3) Opening lot of new window (Ctrl-N)
4) Opening lot of new tab (Ctrl-T)
5) Saving page (requires opening files)
6) Downloading a binary file (requires starting the download manager)
7) Opening successively more difficult windows, e.g. (a) a blank page; (b) a text page with various simple fonts; (c) a text page with small images; (d) a text page with small image and style sheets; (e) a text page with javascript; (f) a text page with image & javascript; (g) a page which requires starting Java (presumably may require resetting the ulimit if it drag in additional shared libraries); (h) any of the various page with streamed or direct video, etc.

In case where the action involves page display it should give me *all* of the page which is possible, e.g. a page with a big image followed by a large number of small image should *still* give me the small image even if the big image will not fit into memory. If there isn't memory for a large Javascript or a Style Sheet, give me a much of the page a you can without them. Etc.

I believe if this stress testing and solution provided it would give people a much better understanding of what page and/or browser activity may be contributing to the so called "memory leak" problems.
