JS debugger: setting breakpoints can be confused by script GC
I think thread actors may return error replies to setBreakpoint packets when they should not, depending on GC timing.

In toolkit/devtools/server/actors/script.js, the function _setBreakpoint calls dbg.findScripts to see if any scripts match the location given in the setBreakpoint request, and returns an error reply if none exist.

Debugger.prototype.findScripts operates by sweeping debuggee compartments looking for scripts that match the query. This means that findScripts can sometimes return scripts that are no longer referenced, but have not yet been recognized as garbage by the garbage collector. Whether findScripts returns such scripts is non-deterministic, as it depends on the relative timing of the debugger and debuggee's activities versus those of the collector.

This is unfortunate, but hard to solve without changes to the Debugger API; short of traversing the object graph itself, findScripts has no way to avoid the problem. (Now that we have Debugger.Source, it might be possible to redesign things to avoid it...) But the non-determinism is documented; caveat implementor, right? :(

I may be misunderstanding script.js, but it seems to me we can return an error when we should not, under the following circumstances:

1) The developer visits the page.
2) In response to user activity, the page loads foo.js.
3) The developer reloads the page. This refresh has not yet loaded foo.js.
4) The developer tries to set a breakpoint on foo.js.

At that point, whether or not the this.dbg.findScripts call in ThreadActor.prototype._setBreakpoint returns any scripts depends on whether foo.js's script was GC'd. If it has been, then the client will be unable to set the breakpoint at all.

Fix: Ignore the protocol spec's discussion of "noScript" errors, and always reply to setBreakpoint packets with success. If no scripts are found, leave the breakpoint in the store to be applied to scripts loaded in the future.